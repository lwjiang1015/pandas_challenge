{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f004-11c2-4d48-a0fa-99536f7ca923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# File to Load\n",
    "school_data = Path(\"Resources/schools_complete.csv\")\n",
    "student_data = Path(\"Resources/students_complete.csv\")\n",
    "\n",
    "# Read School and Student Data Files and store into Pandas DataFrames\n",
    "school_data_df = pd.read_csv(school_data)\n",
    "student_data_df = pd.read_csv(student_data)\n",
    "\n",
    "# Combine the school and student data files into one single dataset.  \n",
    "school_data_merged = pd.merge(student_data_df, school_data_df, how=\"left\", on=[\"school_name\", \"school_name\"])\n",
    "\n",
    "school_data_merged.head(3)\n",
    "\n",
    "# Local Government Area (LGA) Summary\n",
    "# Perform the necessary calculations and create the local government area's key metrics in a DataFrame.\n",
    "\n",
    "Totalnumber_of_uniqueschools = len(school_data_merged[\"school_name\"].unique())\n",
    "Totalnumber_of_uniqueschools\n",
    "\n",
    "Total_students = len(school_data_merged[\"school_name\"])\n",
    "Total_students\n",
    "\n",
    "Total_budget = sum(school_data_merged[\"budget\"].unique())\n",
    "Total_budget\n",
    "\n",
    "Average_maths_score = school_data_merged[\"maths_score\"].mean()\n",
    "Average_maths_score \n",
    "\n",
    "Average_reading_score = school_data_merged[\"reading_score\"].mean()\n",
    "Average_reading_score \n",
    "\n",
    "num_students_passing_maths = len((school_data_merged.loc[school_data_merged[\"maths_score\"]>=50]))\n",
    "num_students_passing_maths\n",
    "percentage_students_passing_maths = (num_students_passing_maths/Total_students)*100\n",
    "percentage_students_passing_maths\n",
    "\n",
    "num_students_passing_reading = len((school_data_merged.loc[school_data_merged[\"reading_score\"]>=50]))\n",
    "num_students_passing_reading\n",
    "percentage_students_passing_reading = (num_students_passing_reading/Total_students)*100\n",
    "percentage_students_passing_reading\n",
    "\n",
    "# filter students with overall passing grades first\n",
    "overall_passing_df = school_data_merged.loc[(school_data_merged[\"maths_score\"]>=50) & (\n",
    "    school_data_merged[\"reading_score\"]>=50), :]\n",
    "overall_passing_df\n",
    "# calculate the % of overall passing\n",
    "percentage_students_passing_overall = (len(overall_passing_df)/Total_students)*100\n",
    "percentage_students_passing_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b13acb-3141-478d-be89-3621f684fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data series for each year\n",
    "# iloc will also allow for conditional statements to filter rows of data\n",
    "# Using iloc on the logic test above only returns rows where the result is True\n",
    "year_9 = school_data_merged[school_data_merged.iloc[:,3] == 9]\n",
    "year_10 = school_data_merged[school_data_merged.iloc[:,3] == 10]\n",
    "year_11 = school_data_merged[school_data_merged.iloc[:,3] == 11]\n",
    "year_12 = school_data_merged[school_data_merged.iloc[:,3] == 12]\n",
    "year_12.head(3)\n",
    "\n",
    "# Convert the Series into DataFrame files if needed\n",
    "# year_9_df = pd.DataFrame(year_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117373b9-b0c1-4a62-87e6-500d070772e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display columns\n",
    "school_data_merged.columns\n",
    "# all calculations by school using school as index\n",
    "# mergeddata_byschools = school_data_merged.set_index(\"school_name\")\n",
    "# mergeddata_byschools.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655063ed-389b-4c05-ae39-fc2758e59441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an overview of the school column\n",
    "school_data_merged['school_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d6ec4-dd9d-4ee1-a830-e8a8b54a9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "# sort by overall passing head (4) and tail (4)\n",
    "\n",
    "# create pandas series for maths scores for each year using conditional statement\n",
    "data_series = pd.Series([\"University of Melbourne\", \"UWA\", \"University of Sydney\",\n",
    "                         \"La Trobe University\", \"University of Technology Sydney\"])\n",
    "# Group each series by school\n",
    "# Combine the series into a dataframe\n",
    "# Convert a single dictionary containing lists into a dataframe\n",
    "pharaoh_df = pd.DataFrame(\n",
    "    {\"Dynasty\": [\"Early Dynastic Period\", \"Old Kingdom\"],\n",
    "     \"Pharaoh\": [\"Thinis\", \"Memphis\"]\n",
    "     }\n",
    ")\n",
    "# Optional: give the displayed data cleaner formatting\n",
    "\n",
    "# # create pandas series for reading scores for each year using conditional statement\n",
    "# Group each series by school\n",
    "# Combine the series into a dataframe\n",
    "# Optional: give the displayed data cleaner formatting\n",
    "\n",
    "# cut () budget bins show performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf9d93-7441-4d4f-9f49-c6ca46f8c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potencially useful functions\n",
    "\n",
    "# Create a file path for the data. \n",
    "comics_path = Path(\"Resources/books_clean.csv\")\n",
    "\n",
    "# Read the modified Comic Books csv and store into Pandas DataFrame\n",
    "comics_df = pd.read_csv(comics_path, encoding=\"utf-8\")\n",
    "comics_df.head()\n",
    "\n",
    "# Reference a single column within a DataFrame\n",
    "data_file_df[\"Amount\"].head()\n",
    "\n",
    "# Reference multiple columns within a DataFrame\n",
    "data_file_df[[\"Amount\", \"Gender\"]].head()\n",
    "\n",
    "# The mean method averages the series\n",
    "average = data_file_df[\"Amount\"].mean()\n",
    "average\n",
    "\n",
    "total = data_file_df[\"Amount\"].sum()\n",
    "total\n",
    "\n",
    "# The value_counts method counts unique values in a column\n",
    "count = data_file_df[\"Gender\"].value_counts()\n",
    "count\n",
    "\n",
    "# Calculations can also be performed on Series and added into DataFrames as new columns\n",
    "thousands_of_dollars = data_file_df[\"Amount\"]/1000\n",
    "data_file_df[\"Thousands of Dollars\"] = thousands_of_dollars\n",
    "\n",
    "# Reorganising the columns using double brackets\n",
    "organised_df = training_df[[\"Name\",\"Trainer\",\"Weight\",\"Membership(Days)\"]]\n",
    "\n",
    "# Using .rename(columns={}) in order to rename columns\n",
    "renamed_df = organised_df.rename(columns={\"Membership(Days)\":\"Membership in Days\", \"Weight\":\"Weight in Pounds\"})\n",
    "\n",
    "# Export file as a CSV, without the Pandas index, but with the header\n",
    "file_one_df.to_csv(\"Output/fileOne.csv\", index=False, header=True)\n",
    "\n",
    "# Push the remade DataFrame to a new CSV file\n",
    "renamed_df.to_csv(\"Output/books_clean.csv\",\n",
    "                  encoding=\"utf-8\", index=False, header=True)\n",
    "\n",
    "# Calculate the number of unique authors in the DataFrame\n",
    "author_count = len(comics_df[\"Author\"].unique())\n",
    "\n",
    "# Calculate the earliest/latest year a book was published\n",
    "earliest_year = comics_df[\"Publication Year\"].min()\n",
    "latest_year = comics_df[\"Publication Year\"].max()\n",
    "\n",
    "# Place all of the data found into a summary DataFrame\n",
    "summary_df = pd.DataFrame({\"Total Unique Authors\": [author_count],\n",
    "                              \"Total Unique Publication Countries\": country_count,\n",
    "                              \"Earliest Year\": earliest_year,\n",
    "                              \"Latest Year\": latest_year})\n",
    "summary_df\n",
    "\n",
    "# Using loc to select all rows for `Common Name` and `Threatened Status`\n",
    "df.loc[:, [\"Common Name\", \"Threatened status\"]].head()\n",
    "\n",
    "# iloc will also allow for conditional statements to filter rows of data\n",
    "# Using iloc on the logic test above only returns rows where the result is True\n",
    "also_only_endangered = df[df.iloc[:,2] == \"Endangered\"]\n",
    "also_only_endangered\n",
    "\n",
    "# Multiple conditions can be set to narrow down or widen the filter\n",
    "only_endangered_and_critical = df.loc[(df[\"Threatened status\"] == \"Endangered\") | (\n",
    "    df[\"Threatened status\"] == \"Critically Endangered\"), :]\n",
    "only_endangered_and_critical\n",
    "\n",
    "# We only like good movies, so find those that scored over 7, and ignore the norm rating\n",
    "good_movies_df = movie_file_df.loc[movie_file_df[\"IMDB\"] > 7, [\n",
    "    \"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "good_movies_df.head()\n",
    "\n",
    "# Finally, export this file to a spread so we can keep track of out new future watch list without the index\n",
    "unknown_movies_df.to_excel(\"output/movieWatchlist.xlsx\", index=False)\n",
    "\n",
    "# Delete column we don't want\n",
    "del employment_data_df['OCCUP_INDEX']\n",
    "\n",
    "# Identify incomplete rows\n",
    "employment_data_df.count()\n",
    "\n",
    "# Drop all rows with missing information\n",
    "employment_data_df = employment_data_df.dropna(how='any')\n",
    "\n",
    "# The YEAR column is the wrong data type. It should be an integer.\n",
    "employment_data_df.dtypes\n",
    "\n",
    "# Use df.astype() method to convert the datatype of the YEAR column\n",
    "employment_data_df = employment_data_df.astype({\"YEAR\": int}, errors='raise')\n",
    "\n",
    "# Verify that the YEAR column datatype has been made an integer\n",
    "employment_data_df['YEAR'].dtype\n",
    "\n",
    "# Display an overview of the OCCUP column\n",
    "employment_data_df['OCCUP'].value_counts()\n",
    "\n",
    "# Clean up OCCUP category. Replace 'Laborer' with 'Labourer',\n",
    "# 'Stone Mason' with 'Stonemason', 'Boot Maker' with 'Bootmaker'\n",
    "# 'Coachtrimmer' with 'Coach Trimmer', and 'None' with 'None at present'\n",
    "employment_data_df['OCCUP'] = employment_data_df['OCCUP'].replace({'Laborer': 'Labourer', \n",
    "                                   'Stone Mason': 'Stonemason', \n",
    "                                   'Boot Maker': 'Bootmaker',\n",
    "                                   'Coachtrimmer': 'Coach Trimmer', \n",
    "                                   'None': 'None at present'})\n",
    "\n",
    "# Display a statistical overview\n",
    "# We can infer the maximum allowable individual contribution from 'max'\n",
    "employment_data_df.describe()\n",
    "\n",
    "# Fill NA values for the column \"SURFACE_TYPE\" with \"Unknown\"\n",
    "road_stops_reduced = road_stops_reduced.fillna({\"SURFACE_TYPE\": \"Unknown\"})\n",
    "road_stops_reduced\n",
    "\n",
    "# Convert the lg_counts Series into a DataFrame\n",
    "lg_inaccessible_stops_df = pd.DataFrame(lg_counts)\n",
    "\n",
    "# Count how many inaccessible road stops are in each local government area\n",
    "grouped_lg_df = road_stops_df.groupby([\"LOCAL_GOVERNMENT_NAME\"])\n",
    "\n",
    "# Get the total \"NUMBER_OF_BINS\", \"NUMBER_OF_TOILETS\", \"NUMBER_OF_TABLES\".\n",
    "grouped_lg_df[[\"NUMBER_OF_BINS\", \"NUMBER_OF_TOILETS\", \"NUMBER_OF_TABLES\"]].sum()\n",
    "\n",
    "# Save toilets and tables sums as series\n",
    "lg_toilets = grouped_lg_df[\"NUMBER_OF_TOILETS\"].sum()\n",
    "lg_tables = grouped_lg_df[\"NUMBER_OF_TABLES\"].sum()\n",
    "\n",
    "# Create a new DataFrame using count and total toilets, and tables\n",
    "lg_summary_df = pd.DataFrame({\"Number of Road Stops with Accessibility Issues\": lg_counts,\n",
    "                              \"Total Toilets\": lg_toilets,\n",
    "                              \"Total Tables\": lg_tables})\n",
    "\n",
    "# It is also possible to group a DataFrame by multiple columns\n",
    "# This returns an object with multiple indexes, however, which can be harder to deal with\n",
    "grouped_lg_surface = road_stops_df.groupby([\"LOCAL_GOVERNMENT_NAME\",\"SURFACE\"])\n",
    "\n",
    "# Converting a GroupBy object into a DataFrame\n",
    "lg_issues_df = pd.DataFrame(\n",
    "    grouped_lg_surface[[\"NUMBER_OF_TOILETS\", \"NUMBER_OF_TABLES\"]].sum())\n",
    "\n",
    "# GroupBy is also useful for situations where you may want to calculate the average\n",
    "amenities_df = road_stops_df[[\"LOCAL_GOVERNMENT_NAME\", \"NUMBER_OF_BINS\", \"NUMBER_OF_TOILETS\", \"NUMBER_OF_TABLES\"]]\n",
    "amenities_df.groupby([\"LOCAL_GOVERNMENT_NAME\"]).mean().head(10)\n",
    "\n",
    "# Merge two DataFrames using an inner join\n",
    "inner_merge_df = pd.merge(info_df, items_df, on=\"customer_id\")\n",
    "outer_merge_df = pd.merge(info_df, items_df, on=\"customer_id\", how=\"outer\")\n",
    "left_merge_df = pd.merge(info_df, items_df, on=\"customer_id\", how=\"left\")\n",
    "right_merge_df = pd.merge(info_df, items_df, on=\"customer_id\", how=\"right\")\n",
    "\n",
    "# Drop NA values from the domestic health expenditure column\n",
    "gdp_expenditure_df = gdp_expenditure_df.dropna(how=\"any\", subset=[\"Domestic Health Expenditure (in millions of USD)\"])\n",
    "\n",
    "# Add a new column that calculates the percentage of GDP spent on domestic health\n",
    "gdp_over_10bil_df[\"Health Expenditure (% GDP)\"] = \\\n",
    "        gdp_over_10bil_df[\"Domestic Health Expenditure (in millions of USD)\"] / \\\n",
    "        gdp_over_10bil_df[\"GDP (in millions of USD)\"] * 100\n",
    "\n",
    "# Sort the data by Health Expenditure (% GDP), Highest to Lowest\n",
    "health_expenditure_sorted_df = gdp_over_10bil_df.sort_values([\"Health Expenditure (% GDP)\"],\n",
    "                                           ascending=False)\n",
    "\n",
    "# Reset Index\n",
    "health_expenditure_sorted_df = health_expenditure_sorted_df.reset_index(drop=True)\n",
    "\n",
    "# Print out the data for the highest percentage of GDP spent on domestic health\n",
    "highest_health_expenditure = health_expenditure_sorted_df.loc[0, :]\n",
    "\n",
    "# Bonus: Print out the data for the lowest % of GDP spent on domestic health with one line of code\n",
    "health_expenditure_sorted_df.loc[len(health_expenditure_sorted_df)-1, :]\n",
    "\n",
    "# Create the bins in which Data will be held\n",
    "# Bins are 0, 59.9, 69.9, 79.9, 89.9, 100.   \n",
    "bins = [0, 59.9, 69.9, 79.9, 89.9, 100]\n",
    "\n",
    "# Create the names for the five bins\n",
    "group_names = [\"F\", \"D\", \"C\", \"B\", \"A\"]\n",
    "\n",
    "# Slice the data and place it into bins\n",
    "test_scores_df[\"Test Score Summary\"] = pd.cut(test_scores_df[\"Test Score\"], bins, labels=group_names, include_lowest=True)\n",
    "\n",
    "# Creating a group based off of the bins\n",
    "test_scores_df = test_scores_df.groupby(\"Test Score Summary\")\n",
    "test_scores_df.max()\n",
    "\n",
    "# Use Map to format all the columns\n",
    "file_df[\"INCOME\"] = file_df[\"INCOME\"].map(\"${:,.2f}\".format)\n",
    "file_df[\"COSTS\"] = file_df[\"COSTS\"].map(\"${:,.2f}\".format)\n",
    "file_df[\"PERCENT30\"] = (file_df[\"PERCENT30\"]*100).map(\"{:.1f}%\".format)\n",
    "file_df[\"PERCENT3050\"] = (file_df[\"PERCENT3050\"]*100).map(\"{:.1f}%\".format)\n",
    "file_df[\"PERCENT50\"] = (file_df[\"PERCENT50\"]*100).map(\"{:.1f}%\".format)\n",
    "file_df[\"PERCENT_NODATA\"] = (file_df[\"PERCENT_NODATA\"]*100).map(\"{:.1f}%\".format)\n",
    "file_df[\"PERCENT_NOBURDEN\"] = (file_df[\"PERCENT_NOBURDEN\"]*100).map(\"{:.1f}%\".format)\n",
    "file_df[\"TOTAL\"] = file_df[\"TOTAL\"].map(\"{:,}\".format)\n",
    "\n",
    "# Collect only those projects that were hosted in Australia.\n",
    "\n",
    "# Create a list of the columns\n",
    "columns = [\"name\", \"goal\", \"pledged\", \"outcome\", \n",
    "    \"country\", \"staff_pick\", \"backers_count\", \"spotlight\"]\n",
    "\n",
    "#  Create a new df for \"AU\" with the columns. \n",
    "hosted_in_aus_df = reduced_crowdfunding_df.loc[reduced_crowdfunding_df[\"country\"] == \"AU\",  columns]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
